<div><b>Data: </b></div><div>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Datasets</div><div>We are focusing on three features; Rent, Crimes, and Commuting time. </div><div>&nbsp;</div><div>1. Rent price by neighborhood. We use Zillow Rental Listings data which shows median rent price by neighborhood and room size. we use rent price for studio by neighborhood level. Accordingly, we use Zillow Neighborhood Boundaries which is a shape file provided by Zillow. we use rent price as of February 2017. We note that rent price data of some neighborhoods are missing, and we treat them as missing (i.e. no interpolation methods). </div><div>&nbsp;</div><div>2. Crime. We can use NYPD Complaint Data Historic which describes every crime with longitude and latitude data by category of crime. In this project, I extract felony crime data as it has serious impacts on neighborhoods and we have enough number of samples for it. The data was generated between 2016-01 and 2018-02. </div><div>&nbsp;</div><div>3. Commuting time to city centers from each neighborhood. As we use zone data for rent, we need to have a representative commuting time for each neighborhood (it is impossible to calculate the time from the infinite number of locations). I calculate the centroid of each polygon (GeoSeries.centroid in python) and define it as an origin. Then, using google map, I calculate commuting time. Google map has a function to calculate the fastest route when we put an origin and a destination. </div><div>We set two destinations; Midtown = Times Square and Financial District = New York Stock Exchange, and took the shortest time to reach destinations by train/bus/walk. To get/scrape the commuting time from each origin to destinations, I used selenium. </div><div>&nbsp;</div><div>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data procurements</div><div>To clean data is the largest part of this project. Basically, I combined data discussed above by join or spatial join. For reproducibility, I note a tricky cleaning process. Zillow Rental Listings data has duplicated but slightly different rows, which should be deleted. For example, we have two Murray Hill in NYC and the data has four Murray Hill wrongly, in which each of them is categorized into the different borough.<span></span></div><div>After the cleaning, we got 64 neighborhoods in which each row has a neighborhood name, rent price, felony counts, commuting times to Times Square and NYSE, and geometry/Polygon. Figure 1 shows the cleaned data in the form of heatmaps. </div>